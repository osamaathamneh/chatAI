# LLMonDBPGAI Backend Environment Configuration
# Copy this file to .env and fill in your actual values

# ============================================================================
# Required: OpenAI API Configuration
# ============================================================================
# Get your API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# ============================================================================
# Required: Database Configuration
# ============================================================================
# Target database URL - the database where SQL queries will be executed
TARGET_DB=postgresql://postgres:password@localhost:5432/your_database

# Catalog database URL - the database where semantic catalog is stored
# If not set, defaults to TARGET_DB
CATALOG_DB=postgresql://postgres:password@localhost:5432/your_database

# ============================================================================
# Required: Semantic Catalog Configuration
# ============================================================================
# Name of the semantic catalog (created with pgai semantic-catalog create)
CATALOG_NAME=production_test

# ============================================================================
# Optional: Model Configuration
# ============================================================================
# Default LLM model to use for SQL generation
# Format: provider:model (e.g., openai:gpt-4o-mini, openai:gpt-5, openrouter:openai/gpt-4o-mini)
DEFAULT_MODEL=openai:gpt-4o-mini

# ============================================================================
# Optional: OpenRouter Configuration
# ============================================================================
# API key for OpenRouter (required if using OpenRouter models)
# Get your API key from https://openrouter.ai/keys
# OPENROUTER_API_KEY=your-openrouter-api-key-here

# ============================================================================
# Optional: Ollama Configuration
# ============================================================================
# Base URL for Ollama (default: http://127.0.0.1:11434)
# Only needed if using Ollama for local LLM inference
OLLAMA_BASE_URL=http://127.0.0.1:11434
